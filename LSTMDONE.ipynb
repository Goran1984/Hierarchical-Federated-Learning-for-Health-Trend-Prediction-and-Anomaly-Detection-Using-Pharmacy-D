{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27ab753-590f-42b8-8c7c-40709232708e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All medication names: ['Medication_28', 'Medication_12', 'Medication_9', 'Medication_11', 'Medication_4', 'Medication_16', 'Medication_13', 'Medication_18', 'Medication_29', 'Medication_8', 'Medication_24', 'Medication_3', 'Medication_6', 'Medication_22', 'Medication_23', 'Medication_30', 'Medication_27', 'Medication_26', 'Medication_25', 'Medication_17', 'Medication_21', 'Medication_1', 'Medication_7', 'Medication_14', 'Medication_10', 'Medication_5', 'Medication_20', 'Medication_15', 'Medication_19', 'Medication_2']\n",
      "Training model for ./Ph01_Z01_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0636\n",
      "Epoch 2/2, Loss: 0.0323\n",
      "Model saved to ./output/Ph01_Z01_C01_train_model.pth\n",
      "Training model for ./Ph01_Z01_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.1088\n",
      "Epoch 2/2, Loss: 0.0318\n",
      "Model saved to ./output/Ph01_Z01_C02_train_model.pth\n",
      "Training model for ./Ph01_Z01_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0556\n",
      "Epoch 2/2, Loss: 0.0300\n",
      "Model saved to ./output/Ph01_Z01_C03_train_model.pth\n",
      "Training model for ./Ph01_Z02_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.1005\n",
      "Epoch 2/2, Loss: 0.0330\n",
      "Model saved to ./output/Ph01_Z02_C01_train_model.pth\n",
      "Training model for ./Ph01_Z02_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0904\n",
      "Epoch 2/2, Loss: 0.0332\n",
      "Model saved to ./output/Ph01_Z02_C02_train_model.pth\n",
      "Training model for ./Ph01_Z02_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0621\n",
      "Epoch 2/2, Loss: 0.0309\n",
      "Model saved to ./output/Ph01_Z02_C03_train_model.pth\n",
      "Training model for ./Ph01_Z03_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0953\n",
      "Epoch 2/2, Loss: 0.0292\n",
      "Model saved to ./output/Ph01_Z03_C01_train_model.pth\n",
      "Training model for ./Ph01_Z03_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.1048\n",
      "Epoch 2/2, Loss: 0.0306\n",
      "Model saved to ./output/Ph01_Z03_C02_train_model.pth\n",
      "Training model for ./Ph01_Z03_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0593\n",
      "Epoch 2/2, Loss: 0.0303\n",
      "Model saved to ./output/Ph01_Z03_C03_train_model.pth\n",
      "Training model for ./Ph02_Z01_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0635\n",
      "Epoch 2/2, Loss: 0.0303\n",
      "Model saved to ./output/Ph02_Z01_C01_train_model.pth\n",
      "Training model for ./Ph02_Z01_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0613\n",
      "Epoch 2/2, Loss: 0.0319\n",
      "Model saved to ./output/Ph02_Z01_C02_train_model.pth\n",
      "Training model for ./Ph02_Z01_C03_train.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nanotec Ss\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nanotec Ss\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.0288\n",
      "Model saved to ./output/Ph02_Z01_C03_train_model.pth\n",
      "Training model for ./Ph02_Z02_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0657\n",
      "Epoch 2/2, Loss: 0.0326\n",
      "Model saved to ./output/Ph02_Z02_C01_train_model.pth\n",
      "Training model for ./Ph02_Z02_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0679\n",
      "Epoch 2/2, Loss: 0.0326\n",
      "Model saved to ./output/Ph02_Z02_C02_train_model.pth\n",
      "Training model for ./Ph02_Z02_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0580\n",
      "Epoch 2/2, Loss: 0.0315\n",
      "Model saved to ./output/Ph02_Z02_C03_train_model.pth\n",
      "Training model for ./Ph02_Z03_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0613\n",
      "Epoch 2/2, Loss: 0.0316\n",
      "Model saved to ./output/Ph02_Z03_C01_train_model.pth\n",
      "Training model for ./Ph02_Z03_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0754\n",
      "Epoch 2/2, Loss: 0.0315\n",
      "Model saved to ./output/Ph02_Z03_C02_train_model.pth\n",
      "Training model for ./Ph02_Z03_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0624\n",
      "Epoch 2/2, Loss: 0.0304\n",
      "Model saved to ./output/Ph02_Z03_C03_train_model.pth\n",
      "Training model for ./Ph03_Z01_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.1018\n",
      "Epoch 2/2, Loss: 0.0319\n",
      "Model saved to ./output/Ph03_Z01_C01_train_model.pth\n",
      "Training model for ./Ph03_Z01_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.1024\n",
      "Epoch 2/2, Loss: 0.0291\n",
      "Model saved to ./output/Ph03_Z01_C02_train_model.pth\n",
      "Training model for ./Ph03_Z01_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0874\n",
      "Epoch 2/2, Loss: 0.0318\n",
      "Model saved to ./output/Ph03_Z01_C03_train_model.pth\n",
      "Training model for ./Ph03_Z02_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0882\n",
      "Epoch 2/2, Loss: 0.0319\n",
      "Model saved to ./output/Ph03_Z02_C01_train_model.pth\n",
      "Training model for ./Ph03_Z02_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0628\n",
      "Epoch 2/2, Loss: 0.0315\n",
      "Model saved to ./output/Ph03_Z02_C02_train_model.pth\n",
      "Training model for ./Ph03_Z02_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.1091\n",
      "Epoch 2/2, Loss: 0.0304\n",
      "Model saved to ./output/Ph03_Z02_C03_train_model.pth\n",
      "Training model for ./Ph03_Z03_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0868\n",
      "Epoch 2/2, Loss: 0.0360\n",
      "Model saved to ./output/Ph03_Z03_C01_train_model.pth\n",
      "Training model for ./Ph03_Z03_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0876\n",
      "Epoch 2/2, Loss: 0.0344\n",
      "Model saved to ./output/Ph03_Z03_C02_train_model.pth\n",
      "Training model for ./Ph03_Z03_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0757\n",
      "Epoch 2/2, Loss: 0.0338\n",
      "Model saved to ./output/Ph03_Z03_C03_train_model.pth\n",
      "Training model for ./Ph04_Z01_C01_train.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nanotec Ss\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nanotec Ss\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.0294\n",
      "Model saved to ./output/Ph04_Z01_C01_train_model.pth\n",
      "Training model for ./Ph04_Z01_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0906\n",
      "Epoch 2/2, Loss: 0.0335\n",
      "Model saved to ./output/Ph04_Z01_C02_train_model.pth\n",
      "Training model for ./Ph04_Z01_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0588\n",
      "Epoch 2/2, Loss: 0.0269\n",
      "Model saved to ./output/Ph04_Z01_C03_train_model.pth\n",
      "Training model for ./Ph04_Z02_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.1159\n",
      "Epoch 2/2, Loss: 0.0336\n",
      "Model saved to ./output/Ph04_Z02_C01_train_model.pth\n",
      "Training model for ./Ph04_Z02_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.0998\n",
      "Epoch 2/2, Loss: 0.0298\n",
      "Model saved to ./output/Ph04_Z02_C02_train_model.pth\n",
      "Training model for ./Ph04_Z02_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.0722\n",
      "Epoch 2/2, Loss: 0.0353\n",
      "Model saved to ./output/Ph04_Z02_C03_train_model.pth\n",
      "Training model for ./Ph04_Z03_C01_train.csv...\n",
      "Epoch 1/2, Loss: 0.0586\n",
      "Epoch 2/2, Loss: 0.0288\n",
      "Model saved to ./output/Ph04_Z03_C01_train_model.pth\n",
      "Training model for ./Ph04_Z03_C02_train.csv...\n",
      "Epoch 1/2, Loss: 0.1026\n",
      "Epoch 2/2, Loss: 0.0298\n",
      "Model saved to ./output/Ph04_Z03_C02_train_model.pth\n",
      "Training model for ./Ph04_Z03_C03_train.csv...\n",
      "Epoch 1/2, Loss: 0.1093\n",
      "Epoch 2/2, Loss: 0.0327\n",
      "Model saved to ./output/Ph04_Z03_C03_train_model.pth\n",
      "City and National level aggregation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define LSTM-based Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step\n",
    "        return out\n",
    "\n",
    "# Dataset Class\n",
    "class PharmacyDataset(Dataset):\n",
    "    def __init__(self, data, features, target):\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[self.features].iloc[idx].values, dtype=torch.float32).unsqueeze(0)  # Add sequence dim\n",
    "        y = torch.tensor(self.data[self.target].iloc[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# Load and Preprocess Data\n",
    "def load_data(file_path, scaler=None):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Feature Engineering: Focus on individual medication costs\n",
    "    data['Medication_Cost'] = data['Cost_Per_Medication'] * data['Quantity']\n",
    "    \n",
    "    features = ['Age', 'Dosage', 'Quantity', 'Medication_Cost']  # Per-medication features\n",
    "    target = 'Medication_Cost'  # Predict the cost per medication\n",
    "    \n",
    "    # Normalize data\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        data[features] = scaler.fit_transform(data[features])\n",
    "    else:\n",
    "        data[features] = scaler.transform(data[features])\n",
    "    \n",
    "    return data, features, target, scaler\n",
    "\n",
    "# Train Model\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Predict with Model (at the medication level)\n",
    "def predict_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in dataloader:\n",
    "            preds = model(x_batch).squeeze(-1)  # Ensure the last dimension is removed\n",
    "            predictions.extend(preds.cpu().numpy())  # Convert to numpy and extend the list\n",
    "    return predictions\n",
    "\n",
    "# Save the trained model\n",
    "def save_model(model, model_path):\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "# Step 1: Gather all unique medication names\n",
    "def gather_medication_names(data_paths):\n",
    "    all_medications = set()\n",
    "    for file_path in data_paths:\n",
    "        data = pd.read_csv(file_path)\n",
    "        all_medications.update(data['Medication_Name'].unique())\n",
    "    return list(all_medications)\n",
    "\n",
    "# Step 2: Train pharmacy models\n",
    "def train_pharmacy_models(data_paths, output_path, scaler=None):\n",
    "    pharmacy_models = []\n",
    "    for file_path in data_paths:\n",
    "        data, features, target, scaler = load_data(file_path, scaler)\n",
    "        dataset_obj = PharmacyDataset(data, features, target)\n",
    "        dataloader = DataLoader(dataset_obj, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Model Initialization\n",
    "        input_size = len(features)\n",
    "        model = LSTMModel(input_size=input_size, hidden_size=64, num_layers=2, output_size=1)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Train Model\n",
    "        print(f\"Training model for {file_path}...\")\n",
    "        train_model(model, dataloader, criterion, optimizer, epochs=2)\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_path = os.path.join(output_path, f\"{os.path.basename(file_path).split('.')[0]}_model.pth\")\n",
    "        save_model(model, model_path)\n",
    "        \n",
    "        pharmacy_models.append(model)\n",
    "    return pharmacy_models\n",
    "\n",
    "# Step 3: Aggregate pharmacy models into one city-level model\n",
    "# Step 3: Aggregate pharmacy models into one city-level model\n",
    "def aggregate_pharmacy_models(pharmacy_models, data_paths, output_path):\n",
    "    # Define mapping of zones to cities (assuming zones 1, 2, and 3 correspond to city 1, city 2, city 3)\n",
    "    city_zones_mapping = {1: ['Ph01_Z01_C01', 'Ph02_Z01_C01', 'Ph03_Z01_C01'],\n",
    "                          2: ['Ph01_Z02_C02', 'Ph02_Z02_C02', 'Ph03_Z02_C02'],\n",
    "                          3: ['Ph01_Z03_C03', 'Ph02_Z03_C03', 'Ph03_Z03_C03']}\n",
    "    \n",
    "    # Initialize dictionary to store city predictions\n",
    "    city_predictions = {f\"City{city_id}\": [] for city_id in range(1, 4)}  # Create dict for city-level predictions\n",
    "    \n",
    "    # Group pharmacy models by city (zones in each city)\n",
    "    for city_id, zones in city_zones_mapping.items():\n",
    "        city_data = []\n",
    "        \n",
    "        # Process each zone in the city\n",
    "        for zone in zones:\n",
    "            # Find the corresponding file for this zone\n",
    "            file_path = next(path for path in data_paths if zone in path)\n",
    "            data, features, target, _ = load_data(file_path)\n",
    "            dataset_obj = PharmacyDataset(data, features, target)\n",
    "            dataloader = DataLoader(dataset_obj, batch_size=32, shuffle=False)\n",
    "            \n",
    "            # Get predictions from the model for this zone\n",
    "            zone_model = pharmacy_models[data_paths.index(file_path)]\n",
    "            predictions = predict_model(zone_model, dataloader)\n",
    "            \n",
    "            # Append predictions to city data\n",
    "            data['Predicted_Cost'] = predictions\n",
    "            data['Predicted_Medication_Name'] = data['Medication_Name']  # Ensure this matches the actual medication\n",
    "            data['Predicted_Trend_Change'] = np.random.randn(len(predictions))  # Example trend changes\n",
    "            city_data.append(data[['Medication_Name', 'Predicted_Cost', 'Predicted_Trend_Change']])\n",
    "        \n",
    "        # Aggregate city data\n",
    "        aggregated_city_data = pd.concat(city_data).drop_duplicates()\n",
    "        \n",
    "        # Just before saving, average the values for each Medication_Name\n",
    "        aggregated_city_data = aggregated_city_data.groupby('Medication_Name').agg({\n",
    "            'Predicted_Cost': 'mean',\n",
    "            'Predicted_Trend_Change': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        city_predictions[f\"City{city_id}\"] = aggregated_city_data\n",
    "    \n",
    "    # For each city, save the aggregated prediction after averaging\n",
    "    for city, city_data in city_predictions.items():\n",
    "        city_data.to_csv(os.path.join(output_path, f\"{city}_predictions.csv\"), index=False)\n",
    "        \n",
    "    return city_predictions\n",
    "\n",
    "# Step 4: Aggregate city-level models into one national-level model\n",
    "def aggregate_city_models(city_predictions, output_path):\n",
    "    national_predictions = pd.concat([city_data for city_data in city_predictions.values()]).drop_duplicates()\n",
    "    \n",
    "    # Just before saving, average the values for each Medication_Name\n",
    "    national_predictions = national_predictions.groupby('Medication_Name').agg({\n",
    "        'Predicted_Cost': 'mean',\n",
    "        'Predicted_Trend_Change': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Save national-level aggregated predictions after averaging\n",
    "    national_predictions.to_csv(os.path.join(output_path, \"national_aggregated_predictions.csv\"), index=False)\n",
    "    return national_predictions\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = \"./\"  # Path where generated datasets are stored\n",
    "    output_path = \"./output/\"  # Path to save outputs\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Define the paths for pharmacy datasets (using format Ph{p:02d}_Z{z:02d}_C{c:02d})\n",
    "    pharmacy_data_paths = [os.path.join(data_path, f\"Ph{p:02d}_Z{z:02d}_C{c:02d}_train.csv\") \n",
    "                           for p in range(1, 5) for z in range(1, 4) for c in range(1, 4)]\n",
    "    \n",
    "    # Step 1: Gather all unique medication names\n",
    "    medication_names = gather_medication_names(pharmacy_data_paths)\n",
    "    print(\"All medication names:\", medication_names)\n",
    "    \n",
    "    # Step 2: Train models for all pharmacies\n",
    "    pharmacy_models = train_pharmacy_models(pharmacy_data_paths, output_path)\n",
    "    \n",
    "    # Step 3: Aggregate predictions for city level\n",
    "    city_predictions = aggregate_pharmacy_models(pharmacy_models, pharmacy_data_paths, output_path)\n",
    "    \n",
    "    # Step 4: Aggregate predictions for national level\n",
    "    national_predictions = aggregate_city_models(city_predictions,output_path)\n",
    "    \n",
    "    # Save the results for each city and the national level\n",
    "    for city, city_data in city_predictions.items():\n",
    "        city_data.to_csv(os.path.join(output_path, f\"{city}_predictions.csv\"), index=False)\n",
    "    \n",
    "    national_predictions.to_csv(os.path.join(output_path, \"national_aggregated_predictions.csv\"), index=False)\n",
    "\n",
    "    print(\"City and National level aggregation complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
